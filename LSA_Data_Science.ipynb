{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalho Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Importando libs **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.common import callMLlibFunc, JavaModelWrapper\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix, DistributedMatrix\n",
    "from pyspark.mllib.linalg import Vectors, DenseMatrix\n",
    "from nltk.stem import SnowballStemmer\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Peso de um termo dado um documento **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def termDocWeight(termFrequencyInDoc, totalTermsInDoc, termFreqInCorpus, totalDocs):\n",
    "    tf = termFrequencyInDoc / totalTermsInDoc\n",
    "    docFreq = totalDocs/termFreqInCorpus\n",
    "    idf = math.log(docFreq)\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Entrando arquivo **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#este arquivo é pré-processado\n",
    "documents = sc.textFile(\"data_works.txt\").map(lambda line: line.split(\" \"))\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Criando dicionário de termos por documento **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#função q retorna um dicionário de termos/frequencia de um documento\n",
    "def caclDocTermFreq(doc):\n",
    "    terms = dict()\n",
    "    for term in doc:\n",
    "        if term in terms:\n",
    "            terms[term] += 1\n",
    "        else:\n",
    "            terms[term] = 1\n",
    "    return terms\n",
    "         \n",
    "#para cada documento, seu dicionário\n",
    "docTermFreqs = documents.map(caclDocTermFreq)\n",
    "\n",
    "#como será usado ao menos mais duas vezes, manteremos em memória\n",
    "docTermFreqs.cache()\n",
    "\n",
    "#\n",
    "docFreqs = docTermFreqs.flatMap(lambda _: _.keys()).map(lambda _: (_, 1)).reduceByKey(lambda x1, x2: x1 + x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Calculando a inversa das frequencias do documento **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#numero de documentos\n",
    "numDocs = docTermFreqs.count()\n",
    "\n",
    "def inverseDocFreq((term, count)):\n",
    "    return (term, math.log(numDocs / count))\n",
    "\n",
    "#idfs é um dicionário termo/inversa\n",
    "idfs = docFreqs.map(inverseDocFreq).collectAsMap()\n",
    "\n",
    "#tdicionário id/termo e o reverso\n",
    "termIds = dict(enumerate(idfs.keys()))\n",
    "idTerms = dict(map(reversed, termIds.iteritems()))\n",
    "\n",
    "docIds = dict(enumerate(['doc'+str(x) for x in range(0,numDocs)]))\n",
    "idDocs = dict(map(reversed, docIds.iteritems()))\n",
    "\n",
    "#Because the term ID map is fairly large and we’ll use it in a few different places, let’s broadcast it along with the IDFs:\n",
    "bIdfs = sc.broadcast(idfs).value\n",
    "bIdTerms = sc.broadcast(idTerms).value\n",
    "bIdDocs = sc.broadcast(idDocs).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TF-IDF para cada documento **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generateVectors(termFreqs):\n",
    "    docTotalTerms = sum(termFreqs.values())\n",
    "    \n",
    "    def calcScores(TF):\n",
    "        filterTF = dict((k,v) for k, v in TF.iteritems() if bIdTerms.has_key(k))\n",
    "        return dict((bIdTerms[k], bIdfs[k] * TF[k] / docTotalTerms) for k, v in filterTF.iteritems())\n",
    "           \n",
    "    return Vectors.sparse(len(bIdTerms), calcScores(termFreqs)) \n",
    "    \n",
    "vecs = docTermFreqs.map(generateVectors)\n",
    "vecs.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** SVD **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SVD(JavaModelWrapper):\n",
    "    \"\"\"Wrapper around the SVD scala case class\"\"\"\n",
    "    @property\n",
    "    def U(self):\n",
    "        \"\"\" Returns a RowMatrix whose columns are the left singular vectors of the SVD if computeU was set to be True.\"\"\"\n",
    "        u = self.call(\"U\")\n",
    "        if u is not None:\n",
    "            return RowMatrix(u)\n",
    "\n",
    "    @property\n",
    "    def s(self):\n",
    "        \"\"\"Returns a DenseVector with singular values in descending order.\"\"\"\n",
    "        return self.call(\"s\")\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        \"\"\" Returns a DenseMatrix whose columns are the right singular vectors of the SVD.\"\"\"\n",
    "        return self.call(\"V\")\n",
    "\n",
    "def computeSVD(row_matrix, k, computeU=True, rCond=1e-9):\n",
    "    \"\"\"\n",
    "    Computes the singular value decomposition of the RowMatrix.\n",
    "    The given row matrix A of dimension (m X n) is decomposed into U * s * V'T where\n",
    "    * s: DenseVector consisting of square root of the eigenvalues (singular values) in descending order.\n",
    "    * U: (m X k) (left singular vectors) is a RowMatrix whose columns are the eigenvectors of (A X A')\n",
    "    * v: (n X k) (right singular vectors) is a Matrix whose columns are the eigenvectors of (A' X A)\n",
    "    :param k: number of singular values to keep. We might return less than k if there are numerically zero singular values.\n",
    "    :param computeU: Whether of not to compute U. If set to be True, then U is computed by A * V * sigma^-1\n",
    "    :param rCond: the reciprocal condition number. All singular values smaller than rCond * sigma(0) are treated as zero, where sigma(0) is the largest singular value.\n",
    "    :returns: SVD object\n",
    "    \"\"\"\n",
    "    \n",
    "    java_model = row_matrix._java_matrix_wrapper.call(\"computeSVD\", int(k), computeU, float(rCond))\n",
    "    return SVD(java_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = RowMatrix (vecs) \n",
    "svd = computeSVD(mat, 50, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encontrando conceitos importantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topTermsInTopConcepts(svd, numConcepts, numTerms, termIds):\n",
    "    v = svd.V\n",
    "    arr = v.toArray().flatten()\n",
    "    topTerms = []\n",
    "\n",
    "    def arrayWithIndex(arr):\n",
    "        tupleList = [ (index,item) for (index,item) in enumerate( arr ) ]\n",
    "        return sorted(tupleList, key=lambda tup: tup[1])[::-1]\n",
    "\n",
    "    def mapList(l):\n",
    "        return [(termIds[x], y) for (x,y) in l]\n",
    "\n",
    "    for i in range(0, numConcepts):\n",
    "        offs = i * v.numRows\n",
    "        termWeights = arrayWithIndex(arr[offs : offs + v.numRows])\n",
    "        topTerms.append(mapList(termWeights[:numTerms]))\n",
    "        \n",
    "    return topTerms\n",
    "\n",
    "topTerms = topTermsInTopConcepts(svd, 50, 400, termIds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top Documents in Top Concepts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topDocsInTopConcepts(svd, numConcepts, numDocs, docIds):\n",
    "    u = svd.U\n",
    "    topDocs = []\n",
    "\n",
    "    def mapListDoc(l):\n",
    "        return [(docIds[y], x) for (x,y) in l]\n",
    "\n",
    "    for i in range(0, numConcepts):\n",
    "        docWeights = u.rows.map(lambda x: x.toArray()[i]).zipWithUniqueId()\n",
    "        topDocs.append(mapListDoc(docWeights.top(numDocs)))\n",
    "    \n",
    "    return topDocs\n",
    "        \n",
    "topDocs = topDocsInTopConcepts(svd, 50, 50, docIds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Consultas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multiplyByDiagonalMatrix(mat, diag):\n",
    "    sArr = diag.toArray()\n",
    "    mult = np.empty([mat.numRows, mat.numCols])\n",
    "    for i in range(0, mat.numRows):\n",
    "        for j in range(0, mat.numCols):\n",
    "            mult[i,j] = mat[i,j]*diag[j]\n",
    "            \n",
    "    return DenseMatrix(mat.numRows, mat.numCols, mult.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multiplyByDiagonalRowMatrix(mat, diag):\n",
    "    sArr = diag.toArray()\n",
    "    matAux = mat.rows.collect()\n",
    "    mult = np.empty([mat.numRows(), mat.numCols()])\n",
    "    for i in range(0, mat.numRows()):\n",
    "        for j in range(0, mat.numCols()):\n",
    "                mult[i,j] = matAux[i][j]*diag[j]\n",
    "            \n",
    "    return RowMatrix(sc.parallelize(mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rowsNormalized(mat):\n",
    "    def calcLenght(row):\n",
    "        sumCols = 0\n",
    "        for col in range(0, mat.numCols):\n",
    "            sumCols += mat[row,col] * mat[row,col]\n",
    "        return math.sqrt(sumCols)\n",
    "    \n",
    "    nomrMat = np.empty([mat.numRows, mat.numCols])\n",
    "    for row in range(0, mat.numRows):\n",
    "        lenght = calcLenght(row)\n",
    "        for col in range(0, mat.numCols):\n",
    "            nomrMat[row, col] = mat[row, col] / lenght\n",
    "    return nomrMat      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distributedRowsNormalized(mat):\n",
    "    matAux = mat.rows.collect()\n",
    "    def calcLenght(row):\n",
    "        sumCols = 0\n",
    "        for col in range(0, mat.numCols()):\n",
    "            sumCols += matAux[row][col] * matAux[row][col]\n",
    "        return math.sqrt(sumCols)\n",
    "    \n",
    "    \n",
    "    nomrMat = np.empty([mat.numRows(), mat.numCols()])\n",
    "    for row in range(0, mat.numRows()):\n",
    "        lenght = calcLenght(row)\n",
    "        for col in range(0, mat.numCols()):\n",
    "            nomrMat[row, col] = matAux[row][col] / lenght\n",
    "    return nomrMat  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topTermsForTerm(normalizedVS, termId):\n",
    "    rowVec = normalizedVS[termId,:]\n",
    "    termScores = [(i,score) for i,score in zip(range(len(rowVec)),normalizedVS.dot(rowVec))]\n",
    "    return sorted(termScores,key = lambda tup: tup[1], reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topDocsForDoc(normalizedUS, docId):\n",
    "    docRowArr = normalizedUS[docId,:]\n",
    "    docScores = normalizedUS.dot(docRowArr)\n",
    "    allDocsWeights = [(i,score) for i,score in zip(range(len(docScores)),docScores)]\n",
    "    return sorted(allDocsWeights, key=lambda tup: tup[1], reverse = True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topDocsForTerm(termId):\n",
    "    rowArr = svd.V.toArray()[termId]\n",
    "    US = multiplyByDiagonalRowMatrix(svd.U,svd.s)\n",
    "    normalizedUS = distributedRowsNormalized(US)\n",
    "    docScores = normalizedUS.dot(rowArr)\n",
    "    allDocsWeights = [(i,score) for i,score in zip(range(len(docScores)),docScores)]\n",
    "    return sorted(allDocsWeights, key=lambda tup: tup[1], reverse = True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topDocsForTermQuery(query):\n",
    "    termRowVec = svd.V.toArray().T.dot(query.toArray())\n",
    "    US = multiplyByDiagonalRowMatrix(svd.U, svd.s)\n",
    "    docScores = np.array(US.rows.collect()).dot(termRowVec)\n",
    "    allDocWeights = sorted([(i, docScores[i]) for i in range(docScores.shape[0])], key=lambda tup: tup[1], reverse=True)\n",
    "    return allDocWeights[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def termsToQueryVector(terms):\n",
    "    indices = [idTerms[stemmer.stem(t)] for t in terms if stemmer.stem(t) in idTerms]\n",
    "    values = [idfs[stemmer.stem(t)] for t in terms if stemmer.stem(t) in idTerms] \n",
    "    return Vectors.sparse(len(idTerms),zip(indices,values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printRelevantTermsForTerm(term):\n",
    "    stemmed = stemmer.stem(term)\n",
    "    if stemmed not in idTerms:\n",
    "        print (\"Termo desconhecido\")\n",
    "    else:\n",
    "        termId = idTerms[stemmed]\n",
    "        VS = multiplyByDiagonalMatrix(svd.V,svd.s)\n",
    "        normalizedVS = rowsNormalized(VS)\n",
    "        topTerms = topTermsForTerm(normalizedVS,termId)\n",
    "\n",
    "        for t in topTerms:\n",
    "            print(termIds[t[0]],' relevancia: ',t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printRelevantDocsForDoc(doc):\n",
    "    US = multiplyByDiagonalRowMatrix(svd.U, svd.s)\n",
    "    normalizedUS = distributedRowsNormalized(US)\n",
    "    \n",
    "    topDocs = topDocsForDoc(normalizedUS, doc)\n",
    "    for d in topDocs:\n",
    "        print(\"Documento: \" + str(d[0]) + \" relevancia: \" + str(d[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printRelevantDocsforTerm(term):\n",
    "    stemmed = stemmer.stem(term)\n",
    "    if stemmed not in idTerms:\n",
    "        print (\"Termo desconhecido\")\n",
    "    else:\n",
    "        termId = idTerms[stemmed]\n",
    "        topDocs = topDocsForTerm(termId)\n",
    "        for d in topDocs:\n",
    "            print('Documento: ' + str(d[0]) + '  relevancia: '+ str(d[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printRelevantDocsForTermQuery(terms):\n",
    "    queryVec = termsToQueryVector(terms)\n",
    "    print(\"Query: \" + str(terms))\n",
    "    for doc, relev in topDocsForTermQuery(queryVec):\n",
    "        print(\"Documento {}, possui relevancia {}\".format(doc, relev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ['hulk', 'incred']\n",
      "Documento 10, possui relevancia 0.296274418708\n",
      "Documento 175, possui relevancia 0.21748956848\n",
      "Documento 27, possui relevancia 0.183700128148\n",
      "Documento 246, possui relevancia 0.157522349283\n",
      "Documento 343, possui relevancia 0.155993612002\n",
      "Documento 35, possui relevancia 0.149376805463\n",
      "Documento 373, possui relevancia 0.141669907733\n",
      "Documento 374, possui relevancia 0.12518879738\n",
      "Documento 370, possui relevancia 0.119144769545\n",
      "Documento 366, possui relevancia 0.0948684925432\n",
      "Documento 152, possui relevancia 0.0679804141064\n",
      "Documento 51, possui relevancia 0.0661146122938\n",
      "Documento 365, possui relevancia 0.0641240306588\n",
      "Documento 283, possui relevancia 0.0579771753193\n",
      "Documento 210, possui relevancia 0.0562385801076\n",
      "Documento 163, possui relevancia 0.051166573553\n",
      "Documento 379, possui relevancia 0.0452439093538\n",
      "Documento 123, possui relevancia 0.0435555481412\n",
      "Documento 377, possui relevancia 0.0417546404629\n",
      "Documento 0, possui relevancia 0.0410603663446\n",
      "Documento 21, possui relevancia 0.0405750750406\n",
      "Documento 414, possui relevancia 0.040103815016\n",
      "Documento 294, possui relevancia 0.0390703339208\n",
      "Documento 291, possui relevancia 0.0386753145864\n",
      "Documento 381, possui relevancia 0.0380764734216\n",
      "Documento 37, possui relevancia 0.037982468122\n",
      "Documento 23, possui relevancia 0.0373730254834\n",
      "Documento 151, possui relevancia 0.0363021991708\n",
      "Documento 20, possui relevancia 0.0336135025873\n",
      "Documento 125, possui relevancia 0.0328466500349\n",
      "Documento 127, possui relevancia 0.0328466500349\n",
      "Documento 360, possui relevancia 0.0321952816915\n",
      "Documento 176, possui relevancia 0.0321129112331\n",
      "Documento 124, possui relevancia 0.0316743377905\n",
      "Documento 267, possui relevancia 0.0302971742937\n",
      "Documento 362, possui relevancia 0.0301499189497\n",
      "Documento 225, possui relevancia 0.030051732155\n",
      "Documento 253, possui relevancia 0.0292110369556\n",
      "Documento 254, possui relevancia 0.0292110368801\n",
      "Documento 265, possui relevancia 0.0284131300262\n",
      "Documento 42, possui relevancia 0.0271466849085\n",
      "Documento 105, possui relevancia 0.0268829179284\n",
      "Documento 413, possui relevancia 0.0255200513049\n",
      "Documento 67, possui relevancia 0.0250786691414\n",
      "Documento 299, possui relevancia 0.0249579135701\n",
      "Documento 358, possui relevancia 0.0249353428568\n",
      "Documento 269, possui relevancia 0.0247691449754\n",
      "Documento 173, possui relevancia 0.0246824760007\n",
      "Documento 326, possui relevancia 0.0243819860834\n",
      "Documento 232, possui relevancia 0.0232479762056\n"
     ]
    }
   ],
   "source": [
    "\n",
    "printRelevantDocsForTermQuery(['hulk', 'incred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py 2)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
