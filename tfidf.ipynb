{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.common import callMLlibFunc, JavaModelWrapper\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix, DistributedMatrix\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Peso de um termo dado um documento **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def termDocWeight(termFrequencyInDoc, totalTermsInDoc, termFreqInCorpus, totalDocs):\n",
    "    tf = termFrequencyInDoc / totalTermsInDoc\n",
    "    docFreq = totalDocs/termFreqInCorpus\n",
    "    idf = math.log(docFreq)\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Entrando arquivo **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#este arquivo é pré-processado\n",
    "\n",
    "documents = sc.textFile(\"data_works.txt\").map(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Criando dicionário de termos por documento **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#função q retorna um dicionário de termos/frequencia de um documento\n",
    "def caclDocTermFreq(doc):\n",
    "    terms = dict()\n",
    "    for term in doc:\n",
    "        if term in terms:\n",
    "            terms[term] += 1\n",
    "        else:\n",
    "            terms[term] = 1\n",
    "    return terms\n",
    "         \n",
    "#para cada documento, seu dicionário\n",
    "docTermFreqs = documents.map(caclDocTermFreq)\n",
    "\n",
    "#como será usado ao menos mais duas vezes, manteremos em memória\n",
    "docTermFreqs.cache()\n",
    "\n",
    "#\n",
    "docFreqs = docTermFreqs.flatMap(lambda _: _.keys()).map(lambda _: (_, 1)).reduceByKey(lambda x1, x2: x1 + x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Calculando a inversa das frequencias do documento **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#numero de documentos\n",
    "numDocs = docTermFreqs.count()\n",
    "\n",
    "def inverseDocFreq((term, count)):\n",
    "    return (term, math.log(numDocs / count))\n",
    "\n",
    "#idfs é um dicionário termo/inversa\n",
    "idfs = docFreqs.map(inverseDocFreq).collectAsMap()\n",
    "\n",
    "#tdicionário id/termo e o reverso\n",
    "termIds = dict(enumerate(idfs.keys()))\n",
    "idTerms = dict(map(reversed, termIds.iteritems()))\n",
    "\n",
    "#Because the term ID map is fairly large and we’ll use it in a few different places, let’s broadcast it along with the IDFs:\n",
    "bIdfs = sc.broadcast(idfs).value\n",
    "bIdTerms = sc.broadcast(idTerms).value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TF-IDF para cada documento **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[279] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generateVectors(termFreqs):\n",
    "    docTotalTerms = sum(termFreqs.values())\n",
    "    \n",
    "    def calcScores(TF):\n",
    "        filterTF = dict((k,v) for k, v in TF.iteritems() if bIdTerms.has_key(k))\n",
    "        return dict((bIdTerms[k], bIdfs[k] * TF[k] / docTotalTerms) for k, v in filterTF.iteritems())\n",
    "           \n",
    "    return Vectors.sparse(len(bIdTerms), calcScores(termFreqs)) \n",
    "    \n",
    "vecs = docTermFreqs.map(generateVectors)\n",
    "vecs.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** SVD **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)\n",
    "\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "\n",
    "#idTerms = idTerms.groupByKey().collect()\n",
    "#print idTerms.collect()\n",
    "#idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "#tfidfIgnore = idfIgnore.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SVD(JavaModelWrapper):\n",
    "    \"\"\"Wrapper around the SVD scala case class\"\"\"\n",
    "    @property\n",
    "    def U(self):\n",
    "        \"\"\" Returns a RowMatrix whose columns are the left singular vectors of the SVD if computeU was set to be True.\"\"\"\n",
    "        u = self.call(\"U\")\n",
    "        if u is not None:\n",
    "            return RowMatrix(u)\n",
    "\n",
    "    @property\n",
    "    def s(self):\n",
    "        \"\"\"Returns a DenseVector with singular values in descending order.\"\"\"\n",
    "        return self.call(\"s\")\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        \"\"\" Returns a DenseMatrix whose columns are the right singular vectors of the SVD.\"\"\"\n",
    "        return self.call(\"V\")\n",
    "\n",
    "def computeSVD(row_matrix, k, computeU=True, rCond=1e-9):\n",
    "    \"\"\"\n",
    "    Computes the singular value decomposition of the RowMatrix.\n",
    "    The given row matrix A of dimension (m X n) is decomposed into U * s * V'T where\n",
    "    * s: DenseVector consisting of square root of the eigenvalues (singular values) in descending order.\n",
    "    * U: (m X k) (left singular vectors) is a RowMatrix whose columns are the eigenvectors of (A X A')\n",
    "    * v: (n X k) (right singular vectors) is a Matrix whose columns are the eigenvectors of (A' X A)\n",
    "    :param k: number of singular values to keep. We might return less than k if there are numerically zero singular values.\n",
    "    :param computeU: Whether of not to compute U. If set to be True, then U is computed by A * V * sigma^-1\n",
    "    :param rCond: the reciprocal condition number. All singular values smaller than rCond * sigma(0) are treated as zero, where sigma(0) is the largest singular value.\n",
    "    :returns: SVD object\n",
    "    \"\"\"\n",
    "    \n",
    "    java_model = row_matrix._java_matrix_wrapper.call(\"computeSVD\", int(k), computeU, float(rCond))\n",
    "    return SVD(java_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = tfidf.collect()\n",
    "#print l\n",
    "rowM = RowMatrix (tfidf) \n",
    "matrixSVD = computeSVD(rowM,True)\n",
    "\n",
    "#for i in l:\n",
    " #   print i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print matrixSVD.U.rows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = matrixSVD.V\n",
    "arr = v.toArray()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "topTerms = []\n",
    "\n",
    "\n",
    "#for i in range(0, v.numCols):\n",
    "    #offs = i * v.numRows\n",
    "    #termWeights = arr[offs : offs+v.numRows]\n",
    "    #sortWeights =  termWeights.sort()\n",
    "    \n",
    "    #sortedWeights = termWeights.sort()\n",
    "    #topTerms.append(sortedWeights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py 2)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
